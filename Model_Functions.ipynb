{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Univariate_Analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Stacking all the Features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
    "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
    "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
    "\n",
    "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\n",
    "train_y = np.array(list(x_train['Class']))\n",
    "\n",
    "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
    "test_y = np.array(list(x_test['Class']))\n",
    "\n",
    "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n",
    "cv_y = np.array(list(x_cv['Class']))\n",
    "\n",
    "\n",
    "train_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\n",
    "test_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\n",
    "cv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n",
    "\n",
    "train_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\n",
    "test_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\n",
    "cv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 One Hot Encoding of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(number of data points * number of features) in train data =  (2124, 55019)\n",
      "(number of data points * number of features) in test data =  (665, 55019)\n",
      "(number of data points * number of features) in cross validation data = (532, 55019)\n"
     ]
    }
   ],
   "source": [
    "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
    "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
    "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Response Encoding of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(number of data points * number of features) in train data =  (2124, 27)\n",
      "(number of data points * number of features) in test data =  (665, 27)\n",
      "(number of data points * number of features) in cross validation data = (532, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\n",
    "print(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\n",
    "print(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding has high dimension than Response encoding. In Response encoding each of the three features are 9 dimensional. The algorithms that GENERALLY work well with HIGH dimensional data are Linear SVM, Logistic Regression, Naive Baye's and the algorithms that generally work well with low dimensional data are Decision Tree, KNN, Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, algo):\n",
    "    algo.fit(train_x, train_y)\n",
    "    sig_algo = CalibratedClassifierCV(algo, method=\"sigmoid\")\n",
    "    sig_algo.fit(train_x, train_y)\n",
    "    pred_y = sig_algo.predict(test_x)\n",
    "\n",
    "    print(\"Log loss :\",log_loss(test_y, sig_algo.predict_proba(test_x)))\n",
    "\n",
    "    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n",
    "    plot_confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_log_loss(train_x, train_y, test_x, test_y,  algo):\n",
    "    algo.fit(train_x, train_y)\n",
    "    sig_algo = CalibratedClassifierCV(algo, method=\"sigmoid\")\n",
    "    sig_algo.fit(train_x, train_y)\n",
    "    sig_algo_probs = sig_algo.predict_proba(test_x)\n",
    "    return log_loss(test_y, sig_algo_probs, eps=1e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below function is specific to Naive Bayes algorithm only. For given indices, we will print the name of the features and check whether the feature is present in the test point's Text or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impfeature_names(indices, text, gene, var, no_features):\n",
    "    gene_count_vec = CountVectorizer()\n",
    "    var_count_vec = CountVectorizer()\n",
    "    text_count_vec = CountVectorizer(min_df=3)\n",
    "    \n",
    "    gene_vec = gene_count_vec.fit(x_train['Gene'])\n",
    "    var_vec  = var_count_vec.fit(x_train['Variation'])\n",
    "    text_vec = text_count_vec.fit(x_train['TEXT'])\n",
    "    \n",
    "    fea1_len = len(gene_vec.get_feature_names())\n",
    "    fea2_len = len(var_count_vec.get_feature_names())\n",
    "    \n",
    "    word_present = 0\n",
    "    for i,v in enumerate(indices):\n",
    "        if (v < fea1_len):\n",
    "            word = gene_vec.get_feature_names()[v]\n",
    "            yes_no = True if word == gene else False\n",
    "            if yes_no:\n",
    "                word_present += 1\n",
    "                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
    "        elif (v < fea1_len+fea2_len):\n",
    "            word = var_vec.get_feature_names()[v-(fea1_len)]\n",
    "            yes_no = True if word == var else False\n",
    "            if yes_no:\n",
    "                word_present += 1\n",
    "                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
    "        else:\n",
    "            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n",
    "            yes_no = True if word in text.split() else False\n",
    "            if yes_no:\n",
    "                word_present += 1\n",
    "                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
    "\n",
    "    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (IntelÂ® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
